{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-17T15:55:38.645981Z",
     "start_time": "2025-03-17T15:55:37.283182Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def display_frames_grid(frames, title=\"Video Sample Frames\" , fps = 16):\n",
    "    \"\"\"\n",
    "    Displays the selected video frames in a grid (4x4).\n",
    "\n",
    "    Args:\n",
    "        frames (np.ndarray): Array of frames with shape (T, H, W, C).\n",
    "        title (str): Title of the grid.\n",
    "    \"\"\"\n",
    "    num_frames = frames.shape[0]\n",
    "    grid_size = int(np.ceil(np.sqrt(num_frames)))  # Approximate square grid\n",
    "\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(10, 10))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_frames:\n",
    "            ax.imshow(frames[i])\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.set_visible(False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def create_gif_from_frames(frames,title, fps=1):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        PIL.Image: Animated GIF image.\n",
    "    \"\"\"\n",
    "    pil_images = [Image.fromarray(frame) for frame in frames]\n",
    "    gif_path = f\"./{title}.gif\"\n",
    "    pil_images[0].save(gif_path, save_all=True, append_images=pil_images[1:], duration=int(1000/fps), loop=0)\n",
    "    return gif_path\n",
    "\n",
    "\n",
    "\n",
    "class VideoFrameDataset(Dataset):\n",
    "    def __init__(self, root_dir, sequence_length=16):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory where the dataset is stored.\n",
    "            sequence_length (int): Number of frames to select uniformly.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.sequence_length = sequence_length\n",
    "        self.video_samples = self._get_all_video_samples()\n",
    "\n",
    "    def _get_all_video_samples(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            list of tuples: Each tuple contains (activity, camera, subject_id, session_id)\n",
    "        \"\"\"\n",
    "        video_samples = []\n",
    "        for activity in os.listdir(self.root_dir):  # Iterate over activities\n",
    "            activity_path = os.path.join(self.root_dir, activity)\n",
    "            if not os.path.isdir(activity_path):\n",
    "                continue  # Skip non-folder items\n",
    "\n",
    "            for camera in os.listdir(activity_path):  # Iterate over cameras\n",
    "                camera_path = os.path.join(activity_path, camera)\n",
    "                if not os.path.isdir(camera_path):\n",
    "                    continue\n",
    "\n",
    "                # Find unique subject-session pairs in this camera folder\n",
    "                all_frames = glob(os.path.join(camera_path, \"*.jpg\"))\n",
    "                subject_sessions = set()\n",
    "\n",
    "                for frame_path in all_frames:\n",
    "                    filename = os.path.basename(frame_path)\n",
    "                    parts = filename.split(\"_\")\n",
    "\n",
    "                    if len(parts) >= 3:\n",
    "                        subject_id, session_id = parts[0], parts[1]\n",
    "                        subject_sessions.add((subject_id, session_id))\n",
    "\n",
    "                # Add all (Activity, Camera, Subject, Session) combinations\n",
    "                for subject_id, session_id in subject_sessions:\n",
    "                    video_samples.append((activity, camera, subject_id, session_id))\n",
    "\n",
    "        return video_samples\n",
    "\n",
    "    def _get_frames_from_video_sample(self, activity, camera, subject_id, session_id):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            list: Sorted list of frame file paths.\n",
    "        \"\"\"\n",
    "        video_sample_path = os.path.join(self.root_dir, activity, camera)\n",
    "        all_frames = sorted(glob(os.path.join(video_sample_path, f\"{subject_id}_{session_id}_*.jpg\")))\n",
    "        return all_frames\n",
    "\n",
    "    def _select_uniform_frames(self, frames):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            list: Selected frame file paths.\n",
    "        \"\"\"\n",
    "        if len(frames) < self.sequence_length:\n",
    "            # Pad by repeating the last frame\n",
    "            frames += [frames[-1]] * (self.sequence_length - len(frames))\n",
    "        else:\n",
    "            step = max(len(frames) // self.sequence_length, 1)\n",
    "            offset = random.randint(0, step - 1) if step > 1 else 0\n",
    "            frames = sorted(frames[i] for i in range(offset, len(frames), step)[:self.sequence_length])\n",
    "\n",
    "        return frames\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of video samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.video_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            np.ndarray: Stacked array of selected frames.\n",
    "            str: Corresponding activity label.\n",
    "            tuple: (subject_id, session_id) for reference.\n",
    "        \"\"\"\n",
    "        activity, camera, subject_id, session_id = self.video_samples[idx]\n",
    "        frames = self._get_frames_from_video_sample(activity, camera, subject_id, session_id)\n",
    "\n",
    "        if not frames:\n",
    "            raise ValueError(f\"No frames found for {activity}/{camera}/{subject_id}_{session_id}\")\n",
    "\n",
    "        selected_frames = self._select_uniform_frames(frames)\n",
    "\n",
    "        # Load and stack frames as NumPy arrays\n",
    "        frame_arrays = [np.array(Image.open(frame).convert('RGB')) for frame in selected_frames]\n",
    "        return np.stack(frame_arrays), activity ,camera, (subject_id, session_id)\n",
    "#Example Usage:\n",
    "#dataset = VideoFrameDataset(root_dir=\"/mnt/Data1/RGB_sd\", sequence_length=16)\n",
    "#print(len(dataset))\n",
    "# gif = create_gif_from_frames(frames, f\"{activity}_{camera}_{subject_id}_{session_id}\")\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Install Library and Load Model",
   "id": "258b2e1f3287941c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T15:55:55.693615Z",
     "start_time": "2025-03-17T15:55:38.651287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install --upgrade -q accelerate bitsandbytes\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "# we need av to be able to read the video\n",
    "!pip install -q av sentencepiece protobuf\n"
   ],
   "id": "d9a9aadfc664d505",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git\r\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-ajq6csxu\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-ajq6csxu\r\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit c8a2b25f915a7745d57c92635415e2517b739bc8\r\n",
      "  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: filelock in /home/gkaviani3/anaconda3/envs/VQA_Bench/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (3.13.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/gkaviani3/anaconda3/envs/VQA_Bench/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (0.29.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/gkaviani3/anaconda3/envs/VQA_Bench/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/gkaviani3/anaconda3/envs/VQA_Bench/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/gkaviani3/anaconda3/envs/VQA_Bench/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/gkaviani3/anaconda3/envs/VQA_Bench/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /home/gkaviani3/anaconda3/envs/VQA_Bench/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/gkaviani3/anaconda3/envs/VQA_Bench/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (0.21.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/gkaviani3/anaconda3/envs/VQA_Bench/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (0.5.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/gkaviani3/anaconda3/envs/VQA_Bench/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (4.67.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/gkaviani3/anaconda3/envs/VQA_Bench/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (2024.12.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/gkaviani3/anaconda3/envs/VQA_Bench/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (4.12.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gkaviani3/anaconda3/envs/VQA_Bench/lib/python3.10/site-packages (from requests->transformers==4.50.0.dev0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gkaviani3/anaconda3/envs/VQA_Bench/lib/python3.10/site-packages (from requests->transformers==4.50.0.dev0) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gkaviani3/anaconda3/envs/VQA_Bench/lib/python3.10/site-packages (from requests->transformers==4.50.0.dev0) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gkaviani3/anaconda3/envs/VQA_Bench/lib/python3.10/site-packages (from requests->transformers==4.50.0.dev0) (2025.1.31)\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T15:55:56.922024Z",
     "start_time": "2025-03-17T15:55:55.798989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n"
   ],
   "id": "474f4d8dfc5e507a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 17 11:55:56 2025       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 3060        On  |   00000000:04:00.0  On |                  N/A |\r\n",
      "| 32%   27C    P8             10W /  170W |     826MiB /  12288MiB |      9%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|    0   N/A  N/A            2626      G   /usr/lib/xorg/Xorg                      305MiB |\r\n",
      "|    0   N/A  N/A            2737    C+G   ...c/gnome-remote-desktop-daemon        104MiB |\r\n",
      "|    0   N/A  N/A            2790      G   /usr/bin/gnome-shell                     45MiB |\r\n",
      "|    0   N/A  N/A            5441      G   ...ess --variations-seed-version        147MiB |\r\n",
      "|    0   N/A  N/A            7079      G   ...ersion=20250316-180048.776000         98MiB |\r\n",
      "|    0   N/A  N/A            8341      G   ...slack/188/usr/lib/slack/slack         44MiB |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "CUDA Available: True\n",
      "CUDA Version: 11.8\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T15:56:14.613202Z",
     "start_time": "2025-03-17T15:55:56.937624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BitsAndBytesConfig, LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor\n",
    "import torch\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "processor = LlavaNextVideoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n",
    "model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/LLaVA-NeXT-Video-7B-hf\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto'\n",
    ")"
   ],
   "id": "9c1b5763ca70fe70",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7088ab17e8b64f6e852b35a1b5bebda9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T16:08:06.544679Z",
     "start_time": "2025-03-17T16:08:06.540834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "def generate_answer(instance, question, processor, model, max_new_tokens=100):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        str: The generated answer from the model.\n",
    "    \"\"\"\n",
    "    # Unpack the sample\n",
    "    frames, activity, camera, (subject_id, session_id) = instance\n",
    "\n",
    "    # Convert each NumPy frame to a PIL Image (as expected by the processor)\n",
    "    to_pil = ToPILImage()\n",
    "    frame_images = [to_pil(frame) for frame in frames]\n",
    "\n",
    "    # Construct the prompt with the provided question\n",
    "    prompt = (\n",
    "        f\"USER: <video>\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"ASSISTANT:\"\n",
    "    )\n",
    "\n",
    "    # Note: both the prompt and video clip must be wrapped in a list.\n",
    "    # inputs = processor(\n",
    "    #     [prompt],\n",
    "    #     videos=[frame_images],\n",
    "    #     padding=True,\n",
    "    #     return_tensors=\"pt\"\n",
    "    # ).to(model.device)\n",
    "    inputs = processor(\n",
    "    videos=[frame_images],\n",
    "    text=[prompt],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "\n",
    "    # Generate an answer from the model\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Decode the generated tokens into a string\n",
    "    answer = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return answer"
   ],
   "id": "9bf1e4510eb52307",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T16:29:31.315762Z",
     "start_time": "2025-03-17T16:29:22.282580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = VideoFrameDataset(root_dir=\"/mnt/Data1/RGB_sd\", sequence_length=16)\n",
    "print(len(dataset))\n",
    "sample = dataset[0]  # This should be a tuple: (frames, activity, camera, (subject_id, session_id))\n",
    "question = \"What is the person doing in this video?\"\n",
    "answer = generate_answer(sample, question, processor, model)\n",
    "print(\"Model Answer:\", answer.split(\"ASSISTANT:\")[1])"
   ],
   "id": "e96b5993793ac66f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885\n",
      "Model Answer:  The person in the video is standing in a kitchen and appears to be using a smartphone or tablet to look up information or possibly browse the internet. They are holding the device in their hands and seem to be focused on the screen. The kitchen is equipped with various appliances and counters, suggesting a domestic or commercial setting where food preparation might take place.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T17:59:17.274213Z",
     "start_time": "2025-03-17T17:59:17.264842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def generate_and_save_answers(dataset, question, processor, model, csv_filename=\"output_answers.csv\", max_new_tokens=100):\n",
    "    \"\"\"\n",
    "    Loops over all dataset instances, generates answers for the given question using the provided\n",
    "    processor and model, and saves the results incrementally to a CSV file.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.exists(csv_filename)\n",
    "\n",
    "    with open(csv_filename, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"activity\", \"camera\", \"subject_id\", \"session_id\", \"question\", \"answer\"])\n",
    "        if not file_exists:\n",
    "            writer.writeheader()  # Write the header only if the file is new\n",
    "\n",
    "        for idx in range(len(dataset)):\n",
    "            try:\n",
    "                instance = dataset[idx]\n",
    "                answer = generate_answer(instance, question, processor, model, max_new_tokens=max_new_tokens)\n",
    "\n",
    "                # Clean up the answer to keep only the part after \"ASSISTANT:\"\n",
    "                cleaned_answer = answer.split(\"ASSISTANT:\")[1]\n",
    "\n",
    "                # Unpack metadata from the instance (frames, activity, camera, (subject_id, session_id))\n",
    "                _, activity, camera, (subject_id, session_id) = instance\n",
    "\n",
    "                # Build the output dictionary\n",
    "                output_entry = {\n",
    "                    \"activity\": activity,\n",
    "                    \"camera\": camera,\n",
    "                    \"subject_id\": subject_id,\n",
    "                    \"session_id\": session_id,\n",
    "                    \"question\": question,\n",
    "                    \"answer\": cleaned_answer\n",
    "                }\n",
    "\n",
    "                # Write the entry immediately to avoid data loss\n",
    "                writer.writerow(output_entry)\n",
    "                f.flush()  # Ensure data is written to disk\n",
    "                print(f\"Processed sample {idx}: {output_entry}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {idx}: {e}\")\n",
    "                continue  # Skip to the next sample instead of stopping\n",
    "\n",
    "    print(\"Completed processing dataset.\")\n",
    "    return csv_filename\n"
   ],
   "id": "cf4ed1150b1326ed",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T16:31:05.029191Z",
     "start_time": "2025-03-17T16:30:08.919393Z"
    }
   },
   "cell_type": "code",
   "source": "generate_and_save_answers(dataset, question, processor, model, csv_filename=\"output_answers.csv\", max_new_tokens=100)",
   "id": "e973a3b5abd3c3e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed sample 0: {'activity': 'Cleaning the kitchen', 'camera': 'camera_2_fps_15', 'subject_id': '05', 'session_id': '3', 'question': 'What is the person doing in this video?', 'answer': ' The person in the video is cleaning a kitchen counter. They are using a sponge and a cloth to wipe down the counter, likely to remove dirt, stains, or food particles from the surface.'}\n",
      "Processed sample 1: {'activity': 'Cleaning the kitchen', 'camera': 'camera_2_fps_15', 'subject_id': '06', 'session_id': '3', 'question': 'What is the person doing in this video?', 'answer': ' The person in the video is cleaning a kitchen counter. They are using a mop to wipe down the counter, and there is a laptop on the counter that is displaying a graphical user interface with a blue background and a white bar at the bottom. The person is wearing a white shirt and appears to be focused on their task.'}\n",
      "Processed sample 2: {'activity': 'Cleaning the kitchen', 'camera': 'camera_2_fps_15', 'subject_id': '09', 'session_id': '4', 'question': 'What is the person doing in this video?', 'answer': ' The person in the video is using a laptop while standing in a kitchen. They appear to be multitasking, possibly working on a task or browsing the internet while also preparing food or attending to other kitchen-related activities. The video does not provide enough context to determine the exact nature of their work or the specific task they are performing.'}\n",
      "Processed sample 3: {'activity': 'Cleaning the kitchen', 'camera': 'camera_2_fps_15', 'subject_id': '09', 'session_id': '3', 'question': 'What is the person doing in this video?', 'answer': ' The person in the video is using a laptop that is placed on a table in a kitchen setting. They are typing on the laptop and looking at the screen, possibly working on a task or browsing the internet. The kitchen appears to be a modern, well-equipped space with various appliances and counters, suggesting a professional or commercial environment.'}\n",
      "Processed sample 4: {'activity': 'Cleaning the kitchen', 'camera': 'camera_2_fps_15', 'subject_id': '17', 'session_id': '4', 'question': 'What is the person doing in this video?', 'answer': ' The person in the video is working at a computer station in a kitchen, likely preparing food or managing a kitchen operation. They are using a laptop and a computer monitor to access information, possibly for recipes, inventory, or other kitchen-related tasks. The kitchen appears to be a commercial or professional setting, with various appliances and equipment in the background.'}\n",
      "Processed sample 5: {'activity': 'Cleaning the kitchen', 'camera': 'camera_2_fps_15', 'subject_id': '02', 'session_id': '4', 'question': 'What is the person doing in this video?', 'answer': ' The person in the video is cleaning a kitchen counter. They are using a mop to wipe down the surface, likely to remove dirt, stains, or spills.'}\n",
      "Processed sample 6: {'activity': 'Cleaning the kitchen', 'camera': 'camera_2_fps_15', 'subject_id': '16', 'session_id': '4', 'question': 'What is the person doing in this video?', 'answer': ' The person in the video is using a laptop that is connected to a camera, possibly for monitoring or recording purposes. They are sitting at a table with a stool, and there is a kitchen area in the background. The person appears to be focused on the laptop screen, possibly working or reviewing something. The setting suggests a workspace or an office environment.'}\n",
      "Processed sample 7: {'activity': 'Cleaning the kitchen', 'camera': 'camera_2_fps_15', 'subject_id': '07', 'session_id': '4', 'question': 'What is the person doing in this video?', 'answer': ' The person in the video is working in a kitchen, specifically in a commercial kitchen setting. They are seen preparing food, possibly in a restaurant or catering environment, and are using a laptop to access information or control a device related to their work. The kitchen is equipped with various appliances and counters, and there are multiple screens displaying different types of information. The person is focused on their task, and the video captures a moment of their workday in this setting.'}\n",
      "Processed sample 8: {'activity': 'Cleaning the kitchen', 'camera': 'camera_2_fps_15', 'subject_id': '17', 'session_id': '3', 'question': 'What is the person doing in this video?', 'answer': ' The person in the video is cleaning a kitchen counter. They are using a sponge to wipe down the counter, and then they move to the other side of the counter to clean it again.'}\n",
      "Processed sample 9: {'activity': 'Cleaning the kitchen', 'camera': 'camera_2_fps_15', 'subject_id': '04', 'session_id': '3', 'question': 'What is the person doing in this video?', 'answer': ' The person in the video is preparing food in a kitchen. They are seen cutting up vegetables, possibly for a salad or a dish that requires chopped vegetables, and are using a knife to do so. The video seems to be focused on cooking or food preparation.'}\n",
      "Processed sample 10: {'activity': 'Cleaning the kitchen', 'camera': 'camera_2_fps_15', 'subject_id': '16', 'session_id': '3', 'question': 'What is the person doing in this video?', 'answer': ' The person in the video appears to be working in a kitchen or a similar setting with a focus on food preparation or cooking. They are using a laptop that is open and displaying a graphical user interface, possibly for a recipe or a cooking app. The individual is also holding a smartphone, which they seem to be using to take a photo or record a video of the food they are preparing. The kitchen is equipped with various appliances and counters, and'}\n",
      "Saved answers to output_answers.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output_answers.csv'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b8df366d6c0f93b5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
